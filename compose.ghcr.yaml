services:
  litellm:
    image: ghcr.io/berriai/litellm:v1.71.1-stable
    ports:
      - "4001:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: ["--config", "/app/config.yaml"]
  llm:
    image: ghcr.io/ggml-org/llama.cpp:server-b5517
    volumes:
      - ./models:/root/.cache/llama.cpp
    ports:
      - "8080:8080"
    expose:
      - 8080
    init: true
    environment:
      - LLAMA_ARG_HF_REPO=bartowski/google_gemma-3-1b-it-qat-GGUF:Q4_K_M
      - LLAMA_ARG_CTX_SIZE=4096
      - LLAMA_ARG_CACHE_REUSE=256
      - LLAMA_ARG_JINJA=1
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -s http://localhost:8080/health | grep -q '\"status\":\"ok\"'"]
      interval: 30s
      timeout: 10s
      retries: 3
  embed:
    image: ghcr.io/ggml-org/llama.cpp:server-b5517
    volumes:
      - ./models:/root/.cache/llama.cpp
    ports:
      - "8081:8080"
    expose:
      - 8080
    init: true
    environment:
      - LLAMA_ARG_HF_REPO=nomic-ai/nomic-embed-text-v2-moe-GGUF:F16
      - LLAMA_ARG_EMBEDDINGS=1
      - LLAMA_ARG_UBATCH=1024
      - LLAMA_ARG_JINJA=1
    healthcheck:
      test: ["CMD", "sh", "-c", "curl -s http://localhost:8080/health | grep -q '\"status\":\"ok\"'"]
      interval: 30s
      timeout: 10s
      retries: 3
  app:
    # Use pre-built image from GitHub Container Registry
    image: ghcr.io/phate334/hipporag-docker:latest
    # Or build locally:
    # build: .
    depends_on:
      llm:
        condition: service_healthy
      embed:
        condition: service_healthy
    environment:
      - OPENAI_BASE_URL=http://litellm:4000/v1
      - OPENAI_API_KEY=sk-template
      - LLM_MODEL_NAME=gemma-3-1b
      - EMBEDDING_MODEL_NAME=nomic-v2-text-embedding
    volumes:
      - ./outputs:/app/outputs
      - ./demo_local.py:/app/demo_local.py
    command: ["demo_local.py"]