services:
  litellm:
    image: ghcr.io/berriai/litellm:v1.71.1-stable
    ports:
      - "4001:4000"
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: ["--config", "/app/config.yaml"]
  llm:
    image: ghcr.io/ggml-org/llama.cpp:server-b5517
    volumes:
      - ./models:/models
    ports:
      - "8080:8080"
    expose:
      - 8080
    init: true
    environment:
      - LLAMA_ARG_MODEL=/models/google_gemma-3-1b-it-qat-Q4_K_M.gguf
      - LLAMA_ARG_CTX_SIZE=4096
      - LLAMA_ARG_CACHE_REUSE=256
      - LLAMA_ARG_JINJA=1
    # command: ["--log-disable"]
  embed:
    image: ghcr.io/ggml-org/llama.cpp:server-b5517
    volumes:
      - ./models:/models
    ports:
      - "8081:8080"
    expose:
      - 8080
    init: true
    environment:
      - LLAMA_ARG_MODEL=/models/nomic-embed-text-v2-moe.f16.gguf
      - LLAMA_ARG_EMBEDDINGS=1
      - LLAMA_ARG_UBATCH=1024
    # command: ["--log-disable"]
  app:
    build: .
    environment:
      - OPENAI_BASE_URL=http://litellm:4000/v1
      - OPENAI_API_KEY=sk-template
      - LLM_MODEL_NAME=gemma-3-1b
      - EMBEDDING_MODEL_NAME=nomic-v2-text-embedding
    volumes:
      - ./outputs:/app/outputs
      - ./demo_local.py:/app/demo_local.py
    command: ["/app/demo_local.py"]
