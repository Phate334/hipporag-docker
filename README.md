# Running HippoRAG in Containers

This project provides a containerized environment for running HippoRAG, leveraging LiteLLM and llama.cpp server to offer a local, OpenAI API-compatible interface. By using Docker containers, you can easily deploy and manage HippoRAG alongside a local LLM backend, enabling efficient and flexible experimentation or development without relying on external APIs.
